{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "67c75f06",
      "metadata": {
        "id": "67c75f06"
      },
      "source": [
        "# Convert tiff files to hdf5 file (for ilastik analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "43fb5a35",
      "metadata": {
        "id": "43fb5a35"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "\n",
        "import tifffile\n",
        "import glob\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff035c54",
      "metadata": {
        "id": "ff035c54"
      },
      "outputs": [],
      "source": [
        "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
        "    \"\"\"\n",
        "    load tiff sequence stored in the same directory\n",
        "    e.g. \n",
        "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
        "    \"\"\"\n",
        "\n",
        "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
        "    imlist.sort() # sort numerically\n",
        "    \n",
        "    if range is not None:\n",
        "        imlist = imlist[ range[0]:range[1]]\n",
        "        \n",
        "    #get image properties by reading the first image\n",
        "    im = tifffile.imread(imlist[0])\n",
        "    imsize_x = im.shape[1]\n",
        "    imsize_y = im.shape[0]\n",
        "    imsize_z = len( imlist )\n",
        "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
        "    imtype = im.dtype\n",
        "    \n",
        "    stack = np.zeros( imsize, dtype=imtype )\n",
        "    for (i, impath) in enumerate(imlist):\n",
        "        im = tifffile.imread( impath )\n",
        "        stack[i,:,:] = im\n",
        "        \n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e96388ef",
      "metadata": {
        "id": "e96388ef"
      },
      "outputs": [],
      "source": [
        "def write_as_hdf5( stack, h5name, destname, \n",
        "                   chunks_enabled=True, chunksize=None,\n",
        "                   attributes=None ):\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
        "    \"\"\"\n",
        "    if chunks_enabled:\n",
        "        if chunksize is None:\n",
        "            chunks = True\n",
        "        else:\n",
        "            chunks = chunksize\n",
        "    else:\n",
        "        chunks = None\n",
        "        \n",
        "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
        "        data = hf.create_dataset (destname,\n",
        "                                  chunks=chunks,\n",
        "                                  data=stack )\n",
        "        if attributes is not None:\n",
        "            for key, value in attributes.items():\n",
        "                data.attrs[key] = value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗\n",
        "Original dataset has 454 images, but it's too large to work on Google Colab, therefore we picked up 201~210th images to reduce the data size."
      ],
      "metadata": {
        "id": "lXPdCQ9qXBRX"
      },
      "id": "lXPdCQ9qXBRX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Download\n",
        "!wget https://www.dropbox.com/s/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
        "!wget https://www.dropbox.com/s/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
        "\n",
        "! unzip 190604_#144_lung_raw_tiff_10slices.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqbh8CUi2627",
        "outputId": "e85ca903-90ee-44e4-fb7c-b63973dec6aa"
      },
      "id": "Dqbh8CUi2627",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-10 06:21:16--  https://www.dropbox.com/s/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip [following]\n",
            "--2022-08-10 06:21:16--  https://www.dropbox.com/s/raw/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucbfd58349ce978e75da49336ada.dl.dropboxusercontent.com/cd/0/inline/Bqs1htxXLIlKdvGXDVObIKNzMnGVQDECUUkGWQi511eNcGGbegBLFyXHKXco8wS8vRBHejhPpoBbv_5YGE0m8gQz0nfobPU_auzTmSLrUCQU2mh3gXpx2PzqASQTm0EN1ERewoNPYPAuttvCaY6rxN_DjM4fnbZYqUvIK3Oojw2qzQ/file# [following]\n",
            "--2022-08-10 06:21:17--  https://ucbfd58349ce978e75da49336ada.dl.dropboxusercontent.com/cd/0/inline/Bqs1htxXLIlKdvGXDVObIKNzMnGVQDECUUkGWQi511eNcGGbegBLFyXHKXco8wS8vRBHejhPpoBbv_5YGE0m8gQz0nfobPU_auzTmSLrUCQU2mh3gXpx2PzqASQTm0EN1ERewoNPYPAuttvCaY6rxN_DjM4fnbZYqUvIK3Oojw2qzQ/file\n",
            "Resolving ucbfd58349ce978e75da49336ada.dl.dropboxusercontent.com (ucbfd58349ce978e75da49336ada.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucbfd58349ce978e75da49336ada.dl.dropboxusercontent.com (ucbfd58349ce978e75da49336ada.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BqtMItIQDUn3WzaFjPa2ub6XESEtt2J1-DL9V98ebK2rjVrkZV0kM6iubrtsuiGRp76EQr-uyXV0NhkyqJMfiEDZp28A4o45GIraE_QqmR-2dtOfDA_SV2G4o1ATAT-f8p2CJiqjMQiwhSgev1BqoBWHNKRZUMIUjCoxTqGpnD_LG3eSWSOOZEf0iQMGOcOyV_UuaI6gTLLu9AbPniCEHq0ZdE19MQHK_nliJkP3gfHcltv2IsPZRf-6gXNiSWSaiXv5xWEZEOE9DOUjUcKNsfr_OkKmXMIs5XWMfP3ASnJ52Ih8ApFf6GggqD6uzXWpqRVe9d3BtYnh2LZcaApAF12IsMjn2-X8nBaKnaQyq4pjX28AbSaVluPJSYuhOGJ-lsFFaTTMXSgJDqFV6Uta1srItN1I6em-PwWifalqd4HLmw/file [following]\n",
            "--2022-08-10 06:21:17--  https://ucbfd58349ce978e75da49336ada.dl.dropboxusercontent.com/cd/0/inline2/BqtMItIQDUn3WzaFjPa2ub6XESEtt2J1-DL9V98ebK2rjVrkZV0kM6iubrtsuiGRp76EQr-uyXV0NhkyqJMfiEDZp28A4o45GIraE_QqmR-2dtOfDA_SV2G4o1ATAT-f8p2CJiqjMQiwhSgev1BqoBWHNKRZUMIUjCoxTqGpnD_LG3eSWSOOZEf0iQMGOcOyV_UuaI6gTLLu9AbPniCEHq0ZdE19MQHK_nliJkP3gfHcltv2IsPZRf-6gXNiSWSaiXv5xWEZEOE9DOUjUcKNsfr_OkKmXMIs5XWMfP3ASnJ52Ih8ApFf6GggqD6uzXWpqRVe9d3BtYnh2LZcaApAF12IsMjn2-X8nBaKnaQyq4pjX28AbSaVluPJSYuhOGJ-lsFFaTTMXSgJDqFV6Uta1srItN1I6em-PwWifalqd4HLmw/file\n",
            "Reusing existing connection to ucbfd58349ce978e75da49336ada.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49084620 (47M) [application/zip]\n",
            "Saving to: ‘190604_#144_lung_raw_tiff_10slices.zip’\n",
            "\n",
            "190604_#144_lung_ra 100%[===================>]  46.81M  78.5MB/s    in 0.6s    \n",
            "\n",
            "2022-08-10 06:21:18 (78.5 MB/s) - ‘190604_#144_lung_raw_tiff_10slices.zip’ saved [49084620/49084620]\n",
            "\n",
            "--2022-08-10 06:21:18--  https://www.dropbox.com/s/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5 [following]\n",
            "--2022-08-10 06:21:18--  https://www.dropbox.com/s/raw/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdd8700e8f102d979046bbd8242.dl.dropboxusercontent.com/cd/0/inline/BqufperrsnSqH8nODlhrsbdN_GartZ9Wlvc7fkTObL4UtienIBge-rNt0LtmzYw2EWBaIzthaLFKHrEGSujYJJZvAwu1iFw9qSxQbvPqAGNKdPBVIOjkqd7QRvTJCTkYMhH8KuvXytUeAcpOrWqQyqXAxq1F4naA4UruJMkX8HtnDA/file# [following]\n",
            "--2022-08-10 06:21:19--  https://ucdd8700e8f102d979046bbd8242.dl.dropboxusercontent.com/cd/0/inline/BqufperrsnSqH8nODlhrsbdN_GartZ9Wlvc7fkTObL4UtienIBge-rNt0LtmzYw2EWBaIzthaLFKHrEGSujYJJZvAwu1iFw9qSxQbvPqAGNKdPBVIOjkqd7QRvTJCTkYMhH8KuvXytUeAcpOrWqQyqXAxq1F4naA4UruJMkX8HtnDA/file\n",
            "Resolving ucdd8700e8f102d979046bbd8242.dl.dropboxusercontent.com (ucdd8700e8f102d979046bbd8242.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucdd8700e8f102d979046bbd8242.dl.dropboxusercontent.com (ucdd8700e8f102d979046bbd8242.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110594048 (105M) [text/plain]\n",
            "Saving to: ‘190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5’\n",
            "\n",
            "190604_P_#144_lung_ 100%[===================>] 105.47M  63.7MB/s    in 1.7s    \n",
            "\n",
            "2022-08-10 06:21:21 (63.7 MB/s) - ‘190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5’ saved [110594048/110594048]\n",
            "\n",
            "Archive:  190604_#144_lung_raw_tiff_10slices.zip\n",
            "   creating: 190604_#144_lung_raw_tiff_10slices/\n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00201]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00202]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00203]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00204]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00205]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00206]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00207]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00208]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00209]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00210]_L[3].tiff  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7b6ce507",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b6ce507",
        "outputId": "f8c27e19-51db-4548-c3cf-fdde1567d36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/190604_#144_lung_raw_tiff_10slices\n",
            "(10, 2160, 2560)\n"
          ]
        }
      ],
      "source": [
        "#Choose Tiff file folder\n",
        "os.chdir(\"/content/190604_#144_lung_raw_tiff_10slices\")\n",
        "print(os.getcwd())\n",
        "\n",
        "#Read Tiff file\n",
        "imgdir = \"/content/190604_#144_lung_raw_tiff_10slices//\"\n",
        "img = load_tiff_sequence( imgdir, imgtype='tiff')\n",
        "\n",
        "print(img.shape)\n",
        "\n",
        "#Save as hdf5\n",
        "filename = \"/content/190604_P_#144_lung_ctrl_x125_639_10slices.hdf5\"\n",
        "dname = \"content\"\n",
        "\n",
        "write_as_hdf5( img, filename, dname, chunks_enabled=True, chunksize=(10,100,100) )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2660bb2",
      "metadata": {
        "id": "c2660bb2"
      },
      "source": [
        "# probability threshold (after ilastik analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "350243f0",
      "metadata": {
        "id": "350243f0"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import tifffile\n",
        "import glob\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f62c5f01",
      "metadata": {
        "id": "f62c5f01"
      },
      "outputs": [],
      "source": [
        "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
        "    \"\"\"\n",
        "    load tiff sequence stored in the same directory\n",
        "    e.g. \n",
        "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
        "    \"\"\"\n",
        "\n",
        "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
        "    imlist.sort() # sort numerically\n",
        "    \n",
        "    if range is not None:\n",
        "        imlist = imlist[ range[0]:range[1]]\n",
        "        \n",
        "    #get image properties by reading the first image\n",
        "    im = tifffile.imread(imlist[0])\n",
        "    imsize_x = im.shape[1]\n",
        "    imsize_y = im.shape[0]\n",
        "    imsize_z = len( imlist )\n",
        "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
        "    imtype = im.dtype\n",
        "    \n",
        "    stack = np.zeros( imsize, dtype=imtype )\n",
        "    for (i, impath) in enumerate(imlist):\n",
        "        im = tifffile.imread( impath )\n",
        "        stack[i,:,:] = im\n",
        "        \n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "38ab0e43",
      "metadata": {
        "id": "38ab0e43"
      },
      "outputs": [],
      "source": [
        "def write_as_hdf5( stack, h5name, destname, \n",
        "                   chunks_enabled=True, chunksize=None,\n",
        "                   attributes=None ):\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
        "    \"\"\"\n",
        "    if chunks_enabled:\n",
        "        if chunksize is None:\n",
        "            chunks = True\n",
        "        else:\n",
        "            chunks = chunksize\n",
        "    else:\n",
        "        chunks = None\n",
        "        \n",
        "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
        "        data = hf.create_dataset (destname,\n",
        "                                  chunks=chunks,\n",
        "                                  data=stack )\n",
        "        if attributes is not None:\n",
        "            for key, value in attributes.items():\n",
        "                data.attrs[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "87aa710e",
      "metadata": {
        "id": "87aa710e"
      },
      "outputs": [],
      "source": [
        "h5name = \"/content/190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5\"\n",
        "hf = h5py.File( h5name, \"r\" )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = hf[\"expmat\"]\n",
        "print (data.shape)\n",
        "l1_prob = data[:,:,:,0] # probability of label1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siTkiqKVEX9v",
        "outputId": "803d1958-e425-4742-e1c4-9b8a9e26efe1"
      },
      "id": "siTkiqKVEX9v",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 2160, 2560, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds = [24, 50, 75, 101, 126, 152, 178, 203, 229]\n",
        "for thresh, percent, idx in zip(thresholds, range(10, 100, 10), list(string.ascii_uppercase)):\n",
        "  # maks a binary mask\n",
        "  binary = (l1_prob > thresh)\n",
        "  print (binary.sum()*8.25*8.25*10)\n",
        "  # make binary into uint8\n",
        "  binary = (255*binary).astype( 'uint16' )\n",
        "  # export as tiff\n",
        "  filename = f\"/content/190604_P_#144_lung_ctrl_x125_639_bin{percent}{idx}.tiff\"\n",
        "  tifffile.imsave( filename, binary )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDrc4ay2YqQQ",
        "outputId": "614573fe-f81b-4d29-d297-bdda6ec0f51e"
      },
      "id": "pDrc4ay2YqQQ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165508942.5\n",
            "133056061.875\n",
            "119030422.5\n",
            "103172540.625\n",
            "85788016.875\n",
            "72314364.375\n",
            "59966465.625\n",
            "49488243.75\n",
            "33390781.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b7d394e",
      "metadata": {
        "id": "0b7d394e"
      },
      "source": [
        "# Count all signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "61d34cb8",
      "metadata": {
        "id": "61d34cb8"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "# from skimage.external import tifffile # Error, use tifffile library\n",
        "import tifffile\n",
        "from scipy.ndimage import label\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.ndimage as ndi\n",
        "import glob\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "eb8e45e2",
      "metadata": {
        "id": "eb8e45e2"
      },
      "outputs": [],
      "source": [
        "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
        "    \"\"\"\n",
        "    load tiff sequence stored in the same directory\n",
        "    e.g. \n",
        "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
        "    \"\"\"\n",
        "\n",
        "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
        "    imlist.sort() # sort numerically\n",
        "    \n",
        "    if range is not None:\n",
        "        imlist = imlist[ range[0]:range[1]]\n",
        "        \n",
        "    #get image properties by reading the first image\n",
        "    im = tifffile.imread(imlist[0])\n",
        "    imsize_x = im.shape[1]\n",
        "    imsize_y = im.shape[0]\n",
        "    imsize_z = len( imlist )\n",
        "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
        "    imtype = im.dtype\n",
        "    \n",
        "    stack = np.zeros( imsize, dtype=imtype )\n",
        "    for (i, impath) in enumerate(imlist):\n",
        "        im = tifffile.imread( impath )\n",
        "        stack[i,:,:] = im\n",
        "        \n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "22e92c5b",
      "metadata": {
        "id": "22e92c5b"
      },
      "outputs": [],
      "source": [
        "def write_as_hdf5( stack, h5name, destname, \n",
        "                   chunks_enabled=True, chunksize=None,\n",
        "                   attributes=None ):\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
        "    \"\"\"\n",
        "    if chunks_enabled:\n",
        "        if chunksize is None:\n",
        "            chunks = True\n",
        "        else:\n",
        "            chunks = chunksize\n",
        "    else:\n",
        "        chunks = None\n",
        "        \n",
        "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
        "        data = hf.create_dataset (destname,\n",
        "                                  chunks=chunks,\n",
        "                                  data=stack )\n",
        "        if attributes is not None:\n",
        "            for key, value in attributes.items():\n",
        "                data.attrs[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9e2d8c89",
      "metadata": {
        "id": "9e2d8c89"
      },
      "outputs": [],
      "source": [
        "def ask_hdf5_size( h5name, dsetname=None ):\n",
        "    \n",
        "    # obtain file handle\n",
        "    hf = h5py.File( h5name, 'r' )\n",
        "    \n",
        "    if dsetname is None:\n",
        "        # get the name of the 0th dataset\n",
        "        dsetname = list( hf.keys() )[0]\n",
        "        dset = hf[ dsetname ]\n",
        "    else:\n",
        "        # get dataset\n",
        "        dset = hf[ dsetname ]\n",
        "    \n",
        "    # print size\n",
        "    print( \"Data set size:\", dset.shape )\n",
        "    \n",
        "    # close handle\n",
        "    hf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "96b8d39e",
      "metadata": {
        "id": "96b8d39e"
      },
      "outputs": [],
      "source": [
        "def load_hdf5( h5name, dsetname=None, multichannel=True ):\n",
        "    \n",
        "    # obtain file handle\n",
        "    hf = h5py.File( h5name, 'r' )\n",
        "    \n",
        "    if dsetname is None:\n",
        "        # get the name of the 0th dataset\n",
        "        dsetname = list( hf.keys() )[0]\n",
        "        dset = hf[ dsetname ]\n",
        "    else:\n",
        "        # get dataset\n",
        "        dset = hf[ dsetname ]\n",
        "    \n",
        "    if multichannel:\n",
        "        # load data as numpy array\n",
        "        data = dset[ :, :, :, 0] # 0th channel = cells\n",
        "        #data = dset[ :, :, :, 0] # 0th channel = cells\n",
        "    else:\n",
        "        data = dset[ :, :, :] # 0th channel = cells\n",
        "        #data = dset[ :, :, :] # 0th channel = cells\n",
        "\n",
        "    # close handle\n",
        "    hf.close()\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6d50b613",
      "metadata": {
        "id": "6d50b613"
      },
      "outputs": [],
      "source": [
        "def calculate_prob_hdf5(file_list, threshold):\n",
        "    \n",
        "    # load probabiltiy image\n",
        "    prob = load_hdf5( file, \"expmat\", multichannel=True )\n",
        "    print (prob.shape)\n",
        "    \n",
        "    ### Binarize probability image\n",
        "    thresh = threshold * 255\n",
        "    binary = ( prob > thresh )\n",
        "    print (\"Total volume of detected signals:\", binary.sum()*8.25*8.25*10)\n",
        "    \n",
        "    # this defines \"connectivity\" between voxels\n",
        "    # structure = ndi.generate_binary_structure( 3, 3 )\n",
        "    \n",
        "    # this defines \"connectivity\" between voxels\n",
        "    structure = np.array( [[[0,0,0],\n",
        "                            [0,0,0],\n",
        "                           [0,0,0]],\n",
        "                           [[0,0,0],\n",
        "                            [0,0,0],\n",
        "                            [0,0,0]],\n",
        "                           [[0,0,0],\n",
        "                            [0,0,0],\n",
        "                            [0,0,0]]])\n",
        "        \n",
        "    # Label isolated objects\n",
        "    objects, num_objects = label( binary, structure )\n",
        "    print( \"Number of detected objects:\", objects.max() )\n",
        "        \n",
        "    # make binary into uint16\n",
        "    binary16 = (255*binary).astype( 'uint16' )\n",
        "    \n",
        "    # export as tiff\n",
        "    #basename = os.path.basename(file)\n",
        "    #filename = rootdir[:-5] + \"tiff/\" + basename[:-4] + \".tif\"\n",
        "    #tifffile.imsave( filename, binary16 )\n",
        "    \n",
        "    ### Find center of mass\n",
        "    ids = np.arange( 1, num_objects+1 )\n",
        "    coms = ndi.center_of_mass( binary, objects, ids )\n",
        "    \n",
        "    # convert to numpy array\n",
        "    coms = np.array( coms )\n",
        "    \n",
        "    # Compute volume of each object\n",
        "    unique, counts = np.unique( objects, return_counts=True )\n",
        "    # remove 0\n",
        "    unique = unique[1:]\n",
        "    counts = counts[1:]\n",
        "    \n",
        "    # create empty dataframe\n",
        "    df = pd.DataFrame()\n",
        "    \n",
        "    # colum \"ID\"\n",
        "    df['ID'] = unique\n",
        "    \n",
        "    # column \"X\", \"Y\", \"Z\"\n",
        "    df['X'] = coms[ :, 2 ]\n",
        "    df['Y'] = coms[ :, 1 ]\n",
        "    df['Z'] = coms[ :, 0 ]\n",
        "    \n",
        "    # colum \"volume\"\n",
        "    df[\"volume\"] = counts\n",
        "    \n",
        "    # save as csv\n",
        "    basename = os.path.basename(file)\n",
        "    csvdir = rootdir + \"csv\"\n",
        "    if not os.path.exists(csvdir):\n",
        "      os.mkdir(csvdir)\n",
        "    filename = rootdir + \"csv/\" + basename[:-26] + f\"_p{int(threshold*100)}_all_639.csv\"\n",
        "    df.to_csv( filename, index=False, float_format='%.2f' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bc2b2c2a",
      "metadata": {
        "id": "bc2b2c2a"
      },
      "outputs": [],
      "source": [
        "# Define root diretory\n",
        "rootdir = \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9c9f38c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c9f38c6",
        "outputId": "fde61533-78ed-4245-c175-1b43cda2eb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5']\n"
          ]
        }
      ],
      "source": [
        "# get files which ends with 'probability'\n",
        "file_list = glob.glob( rootdir + \"*_Probabilities_10slices.h5\" )\n",
        "print( file_list )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = file_list[0]\n",
        "prob = load_hdf5( file, \"expmat\", multichannel=False )"
      ],
      "metadata": {
        "id": "-lGLhY7DGom6"
      },
      "id": "-lGLhY7DGom6",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3176aea9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3176aea9",
        "outputId": "d06bc827-236c-403d-9777-7ab1e1adb2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 160266768.75\n",
            "Number of detected objects: 235470\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 117569120.625\n",
            "Number of detected objects: 172737\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 84283835.625\n",
            "Number of detected objects: 123833\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 59966465.625\n",
            "Number of detected objects: 88105\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 33390781.875\n",
            "Number of detected objects: 49059\n"
          ]
        }
      ],
      "source": [
        "# loop through all files and thresholds\n",
        "thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "for file in file_list:\n",
        "    for thresh in thresholds:\n",
        "      ask_hdf5_size( file, dsetname=None )\n",
        "      raw = load_hdf5(file, multichannel=False)\n",
        "      print (file.rsplit(\"/\")[-1][:-3])\n",
        "      calculate_prob_hdf5(file, thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f916299f",
      "metadata": {
        "id": "f916299f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Signal extraction 10slices.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}