{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c75f06",
   "metadata": {},
   "source": [
    "# Convert tiff files to hdf5 file (for ilastik analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from skimage.external import tifffile\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff035c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
    "    \"\"\"\n",
    "    load tiff sequence stored in the same directory\n",
    "    e.g. \n",
    "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
    "    \"\"\"\n",
    "\n",
    "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
    "    imlist.sort() # sort numerically\n",
    "    \n",
    "    if range is not None:\n",
    "        imlist = imlist[ range[0]:range[1]]\n",
    "        \n",
    "    #get image properties by reading the first image\n",
    "    im = tifffile.imread(imlist[0])\n",
    "    imsize_x = im.shape[1]\n",
    "    imsize_y = im.shape[0]\n",
    "    imsize_z = len( imlist )\n",
    "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
    "    imtype = im.dtype\n",
    "    \n",
    "    stack = np.zeros( imsize, dtype=imtype )\n",
    "    for (i, impath) in enumerate(imlist):\n",
    "        im = tifffile.imread( impath )\n",
    "        stack[i,:,:] = im\n",
    "        \n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96388ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_as_hdf5( stack, h5name, destname, \n",
    "                   chunks_enabled=True, chunksize=None,\n",
    "                   attributes=None ):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
    "    \"\"\"\n",
    "    if chunks_enabled:\n",
    "        if chunksize is None:\n",
    "            chunks = True\n",
    "        else:\n",
    "            chunks = chunksize\n",
    "    else:\n",
    "        chunks = None\n",
    "        \n",
    "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
    "        data = hf.create_dataset (destname,\n",
    "                                  chunks=chunks,\n",
    "                                  data=stack )\n",
    "        if attributes is not None:\n",
    "            for key, value in attributes.items():\n",
    "                data.attrs[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ce507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose Tiff file folder\n",
    "os.chdir(\"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639\")\n",
    "print(os.getcwd())\n",
    "\n",
    "#Read Tiff file\n",
    "imgdir = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639//\"\n",
    "img = load_tiff_sequence( imgdir, imgtype='tiff')\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "#Save as hdf5\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639.hdf5\"\n",
    "dname = \"LSFM\"\n",
    "\n",
    "write_as_hdf5( img, filename, dname, chunks_enabled=True, chunksize=(100,100,100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2660bb2",
   "metadata": {},
   "source": [
    "# probability threshold (after ilastik analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350243f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from skimage.external import tifffile\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
    "    \"\"\"\n",
    "    load tiff sequence stored in the same directory\n",
    "    e.g. \n",
    "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
    "    \"\"\"\n",
    "\n",
    "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
    "    imlist.sort() # sort numerically\n",
    "    \n",
    "    if range is not None:\n",
    "        imlist = imlist[ range[0]:range[1]]\n",
    "        \n",
    "    #get image properties by reading the first image\n",
    "    im = tifffile.imread(imlist[0])\n",
    "    imsize_x = im.shape[1]\n",
    "    imsize_y = im.shape[0]\n",
    "    imsize_z = len( imlist )\n",
    "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
    "    imtype = im.dtype\n",
    "    \n",
    "    stack = np.zeros( imsize, dtype=imtype )\n",
    "    for (i, impath) in enumerate(imlist):\n",
    "        im = tifffile.imread( impath )\n",
    "        stack[i,:,:] = im\n",
    "        \n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_as_hdf5( stack, h5name, destname, \n",
    "                   chunks_enabled=True, chunksize=None,\n",
    "                   attributes=None ):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
    "    \"\"\"\n",
    "    if chunks_enabled:\n",
    "        if chunksize is None:\n",
    "            chunks = True\n",
    "        else:\n",
    "            chunks = chunksize\n",
    "    else:\n",
    "        chunks = None\n",
    "        \n",
    "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
    "        data = hf.create_dataset (destname,\n",
    "                                  chunks=chunks,\n",
    "                                  data=stack )\n",
    "        if attributes is not None:\n",
    "            for key, value in attributes.items():\n",
    "                data.attrs[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5name = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_Probabilities.h5\"\n",
    "hf = h5py.File( h5name, \"r\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary24 = (l1_prob > 24)\n",
    "print (binary24.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary24 = (255*binary24).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin10A.tiff\"\n",
    "tifffile.imsave( filename, binary24 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary50 = (l1_prob > 50)\n",
    "print (binary50.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary50 = (255*binary50).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin20B.tiff\"\n",
    "tifffile.imsave( filename, binary50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary75 = (l1_prob > 75)\n",
    "print (binary75.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary75 = (255*binary75).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin30C.tiff\"\n",
    "tifffile.imsave( filename, binary75 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7db4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary101 = (l1_prob > 101)\n",
    "print (binary101.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary101 = (255*binary101).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin40D.tiff\"\n",
    "tifffile.imsave( filename, binary101 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ec412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary126 = (l1_prob > 126)\n",
    "print (binary126.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary126 = (255*binary126).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin50E.tiff\"\n",
    "tifffile.imsave( filename, binary126 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary152 = (l1_prob > 152)\n",
    "print (binary152.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary152 = (255*binary152).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin60F.tiff\"\n",
    "tifffile.imsave( filename, binary152 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebffa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary178 = (l1_prob > 178)\n",
    "print (binary178.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary178 = (255*binary178).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin70G.tiff\"\n",
    "tifffile.imsave( filename, binary178 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29487e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary203 = (l1_prob > 203)\n",
    "print (binary203.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary203 = (255*binary203).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin80H.tiff\"\n",
    "tifffile.imsave( filename, binary203 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maks a binary mask\n",
    "binary229 = (l1_prob > 229)\n",
    "print (binary229.sum()*8.25*8.25*10)\n",
    "# make binary into uint8\n",
    "binary229 = (255*binary229).astype( 'uint16' )\n",
    "# export as tiff\n",
    "filename = \"/media/betalab/Samsung_T5/LSFM/190604_P_#144_lung_ctrl_x125_639_bin90I.tiff\"\n",
    "tifffile.imsave( filename, binary229 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d394e",
   "metadata": {},
   "source": [
    "# Count all signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d34cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from skimage.external import tifffile\n",
    "from scipy.ndimage import label\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.ndimage as ndi\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
    "    \"\"\"\n",
    "    load tiff sequence stored in the same directory\n",
    "    e.g. \n",
    "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
    "    \"\"\"\n",
    "\n",
    "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
    "    imlist.sort() # sort numerically\n",
    "    \n",
    "    if range is not None:\n",
    "        imlist = imlist[ range[0]:range[1]]\n",
    "        \n",
    "    #get image properties by reading the first image\n",
    "    im = tifffile.imread(imlist[0])\n",
    "    imsize_x = im.shape[1]\n",
    "    imsize_y = im.shape[0]\n",
    "    imsize_z = len( imlist )\n",
    "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
    "    imtype = im.dtype\n",
    "    \n",
    "    stack = np.zeros( imsize, dtype=imtype )\n",
    "    for (i, impath) in enumerate(imlist):\n",
    "        im = tifffile.imread( impath )\n",
    "        stack[i,:,:] = im\n",
    "        \n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e92c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_as_hdf5( stack, h5name, destname, \n",
    "                   chunks_enabled=True, chunksize=None,\n",
    "                   attributes=None ):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
    "    \"\"\"\n",
    "    if chunks_enabled:\n",
    "        if chunksize is None:\n",
    "            chunks = True\n",
    "        else:\n",
    "            chunks = chunksize\n",
    "    else:\n",
    "        chunks = None\n",
    "        \n",
    "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
    "        data = hf.create_dataset (destname,\n",
    "                                  chunks=chunks,\n",
    "                                  data=stack )\n",
    "        if attributes is not None:\n",
    "            for key, value in attributes.items():\n",
    "                data.attrs[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_hdf5_size( h5name, dsetname=None ):\n",
    "    \n",
    "    # obtain file handle\n",
    "    hf = h5py.File( h5name, 'r' )\n",
    "    \n",
    "    if dsetname is None:\n",
    "        # get the name of the 0th dataset\n",
    "        dsetname = list( hf.keys() )[0]\n",
    "        dset = hf[ dsetname ]\n",
    "    else:\n",
    "        # get dataset\n",
    "        dset = hf[ dsetname ]\n",
    "    \n",
    "    # print size\n",
    "    print( \"Data set size:\", dset.shape )\n",
    "    \n",
    "    # close handle\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5( h5name, dsetname=None, multichannel=True ):\n",
    "    \n",
    "    # obtain file handle\n",
    "    hf = h5py.File( h5name, 'r' )\n",
    "    \n",
    "    if dsetname is None:\n",
    "        # get the name of the 0th dataset\n",
    "        dsetname = list( hf.keys() )[0]\n",
    "        dset = hf[ dsetname ]\n",
    "    else:\n",
    "        # get dataset\n",
    "        dset = hf[ dsetname ]\n",
    "    \n",
    "    if multichannel:\n",
    "        # load data as numpy array\n",
    "        data = dset[ :, :, :, 0] # 0th channel = cells\n",
    "        #data = dset[ :, :, :, 0] # 0th channel = cells\n",
    "    else:\n",
    "        data = dset[ :, :, :] # 0th channel = cells\n",
    "        #data = dset[ :, :, :] # 0th channel = cells\n",
    "\n",
    "    # close handle\n",
    "    hf.close()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob_hdf5(file_list):\n",
    "    \n",
    "    # load probabiltiy image\n",
    "    prob = load_hdf5( file, \"exported_data\", multichannel=True )\n",
    "    print (prob.shape)\n",
    "    \n",
    "    ### Binarize probability image\n",
    "    thresh = 0.9 * 255\n",
    "    binary = ( prob > thresh )\n",
    "    print (\"Total volume of detected signals:\", binary.sum()*8.25*8.25*10)\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    # structure = ndi.generate_binary_structure( 3, 3 )\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    structure = np.array( [[[0,0,0],\n",
    "                            [0,0,0],\n",
    "                           [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]]])\n",
    "        \n",
    "    # Label isolated objects\n",
    "    objects, num_objects = label( binary, structure )\n",
    "    print( \"Number of detected objects:\", objects.max() )\n",
    "        \n",
    "    # make binary into uint16\n",
    "    binary16 = (255*binary).astype( 'uint16' )\n",
    "    \n",
    "    # export as tiff\n",
    "    #basename = os.path.basename(file)\n",
    "    #filename = rootdir[:-5] + \"tiff/\" + basename[:-4] + \".tif\"\n",
    "    #tifffile.imsave( filename, binary16 )\n",
    "    \n",
    "    ### Find center of mass\n",
    "    ids = np.arange( 1, num_objects+1 )\n",
    "    coms = ndi.center_of_mass( binary, objects, ids )\n",
    "    \n",
    "    # convert to numpy array\n",
    "    coms = np.array( coms )\n",
    "    \n",
    "    # Compute volume of each object\n",
    "    unique, counts = np.unique( objects, return_counts=True )\n",
    "    # remove 0\n",
    "    unique = unique[1:]\n",
    "    counts = counts[1:]\n",
    "    \n",
    "    # create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # colum \"ID\"\n",
    "    df['ID'] = unique\n",
    "    \n",
    "    # column \"X\", \"Y\", \"Z\"\n",
    "    df['X'] = coms[ :, 2 ]\n",
    "    df['Y'] = coms[ :, 1 ]\n",
    "    df['Z'] = coms[ :, 0 ]\n",
    "    \n",
    "    # colum \"volume\"\n",
    "    df[\"volume\"] = counts\n",
    "    \n",
    "    # save as csv\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"csv/\" + basename[:-20] + \"_p90_all_639.csv\"\n",
    "    df.to_csv( filename, index=False, float_format='%.2f' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root diretory\n",
    "rootdir = \"I:\\Sup4\\hdf5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files which ends with 'probability'\n",
    "file_list = glob.glob( rootdir + \"*_Probabilities.h5\" )\n",
    "print( file_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all files\n",
    "for file in file_list:\n",
    "    ask_hdf5_size( file, dsetname=None )\n",
    "    raw = load_hdf5(file, multichannel=False)\n",
    "    print (file[50:-3])\n",
    "    calculate_prob_hdf5(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ee2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob_hdf5(file_list):\n",
    "    \n",
    "    # load probabiltiy image\n",
    "    prob = load_hdf5( file, \"exported_data\", multichannel=True )\n",
    "    print (prob.shape)\n",
    "    \n",
    "    ### Binarize probability image\n",
    "    thresh = 0.7 * 255\n",
    "    binary = ( prob > thresh )\n",
    "    print (\"Total volume of detected signals:\", binary.sum()*8.25*8.25*10)\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    # structure = ndi.generate_binary_structure( 3, 3 )\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    structure = np.array( [[[0,0,0],\n",
    "                            [0,0,0],\n",
    "                           [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]]])\n",
    "        \n",
    "    # Label isolated objects\n",
    "    objects, num_objects = label( binary, structure )\n",
    "    print( \"Number of detected objects:\", objects.max() )\n",
    "        \n",
    "    # make binary into uint16\n",
    "    binary16 = (255*binary).astype( 'uint16' )\n",
    "    \n",
    "    # export as tiff\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"tiff/\" + basename[:-4] + \".tif\"\n",
    "    tifffile.imsave( filename, binary16 )\n",
    "    \n",
    "    ### Find center of mass\n",
    "    ids = np.arange( 1, num_objects+1 )\n",
    "    coms = ndi.center_of_mass( binary, objects, ids )\n",
    "    \n",
    "    # convert to numpy array\n",
    "    coms = np.array( coms )\n",
    "    \n",
    "    # Compute volume of each object\n",
    "    unique, counts = np.unique( objects, return_counts=True )\n",
    "    # remove 0\n",
    "    unique = unique[1:]\n",
    "    counts = counts[1:]\n",
    "    \n",
    "    # create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # colum \"ID\"\n",
    "    df['ID'] = unique\n",
    "    \n",
    "    # column \"X\", \"Y\", \"Z\"\n",
    "    df['X'] = coms[ :, 2 ]\n",
    "    df['Y'] = coms[ :, 1 ]\n",
    "    df['Z'] = coms[ :, 0 ]\n",
    "    \n",
    "    # colum \"volume\"\n",
    "    df[\"volume\"] = counts\n",
    "    \n",
    "    # save as csv\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"csv/\" + basename[:-20] + \"_p70_all_639.csv\"\n",
    "    df.to_csv( filename, index=False, float_format='%.2f' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root diretory\n",
    "rootdir = \"I:\\Sup4\\hdf5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432480f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files which ends with 'probability'\n",
    "file_list = glob.glob( rootdir + \"*_Probabilities.h5\" )\n",
    "print( file_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all files\n",
    "for file in file_list:\n",
    "    ask_hdf5_size( file, dsetname=None )\n",
    "    raw = load_hdf5(file, multichannel=False)\n",
    "    print (file[50:-3])\n",
    "    calculate_prob_hdf5(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4271053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob_hdf5(file_list):\n",
    "    \n",
    "    # load probabiltiy image\n",
    "    prob = load_hdf5( file, \"exported_data\", multichannel=True )\n",
    "    print (prob.shape)\n",
    "    \n",
    "    ### Binarize probability image\n",
    "    thresh = 0.5 * 255\n",
    "    binary = ( prob > thresh )\n",
    "    print (\"Total volume of detected signals:\", binary.sum()*8.25*8.25*10)\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    # structure = ndi.generate_binary_structure( 3, 3 )\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    structure = np.array( [[[0,0,0],\n",
    "                            [0,0,0],\n",
    "                           [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]]])\n",
    "        \n",
    "    # Label isolated objects\n",
    "    objects, num_objects = label( binary, structure )\n",
    "    print( \"Number of detected objects:\", objects.max() )\n",
    "        \n",
    "    # make binary into uint16\n",
    "    binary16 = (255*binary).astype( 'uint16' )\n",
    "    \n",
    "    # export as tiff\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"tiff/\" + basename[:-4] + \".tif\"\n",
    "    tifffile.imsave( filename, binary16 )\n",
    "    \n",
    "    ### Find center of mass\n",
    "    ids = np.arange( 1, num_objects+1 )\n",
    "    coms = ndi.center_of_mass( binary, objects, ids )\n",
    "    \n",
    "    # convert to numpy array\n",
    "    coms = np.array( coms )\n",
    "    \n",
    "    # Compute volume of each object\n",
    "    unique, counts = np.unique( objects, return_counts=True )\n",
    "    # remove 0\n",
    "    unique = unique[1:]\n",
    "    counts = counts[1:]\n",
    "    \n",
    "    # create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # colum \"ID\"\n",
    "    df['ID'] = unique\n",
    "    \n",
    "    # column \"X\", \"Y\", \"Z\"\n",
    "    df['X'] = coms[ :, 2 ]\n",
    "    df['Y'] = coms[ :, 1 ]\n",
    "    df['Z'] = coms[ :, 0 ]\n",
    "    \n",
    "    # colum \"volume\"\n",
    "    df[\"volume\"] = counts\n",
    "    \n",
    "    # save as csv\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"csv/\" + basename[:-20] + \"_p50_all_639.csv\"\n",
    "    df.to_csv( filename, index=False, float_format='%.2f' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55640350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root diretory\n",
    "rootdir = \"I:\\Sup4\\hdf5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4004662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files which ends with 'probability'\n",
    "file_list = glob.glob( rootdir + \"*_Probabilities.h5\" )\n",
    "print( file_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751427b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all files\n",
    "for file in file_list:\n",
    "    ask_hdf5_size( file, dsetname=None )\n",
    "    raw = load_hdf5(file, multichannel=False)\n",
    "    print (file[50:-3])\n",
    "    calculate_prob_hdf5(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob_hdf5(file_list):\n",
    "    \n",
    "    # load probabiltiy image\n",
    "    prob = load_hdf5( file, \"exported_data\", multichannel=True )\n",
    "    print (prob.shape)\n",
    "    \n",
    "    ### Binarize probability image\n",
    "    thresh = 0.3 * 255\n",
    "    binary = ( prob > thresh )\n",
    "    print (\"Total volume of detected signals:\", binary.sum()*8.25*8.25*10)\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    # structure = ndi.generate_binary_structure( 3, 3 )\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    structure = np.array( [[[0,0,0],\n",
    "                            [0,0,0],\n",
    "                           [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]]])\n",
    "        \n",
    "    # Label isolated objects\n",
    "    objects, num_objects = label( binary, structure )\n",
    "    print( \"Number of detected objects:\", objects.max() )\n",
    "        \n",
    "    # make binary into uint16\n",
    "    binary16 = (255*binary).astype( 'uint16' )\n",
    "    \n",
    "    # export as tiff\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"tiff/\" + basename[:-4] + \".tif\"\n",
    "    tifffile.imsave( filename, binary16 )\n",
    "    \n",
    "    ### Find center of mass\n",
    "    ids = np.arange( 1, num_objects+1 )\n",
    "    coms = ndi.center_of_mass( binary, objects, ids )\n",
    "    \n",
    "    # convert to numpy array\n",
    "    coms = np.array( coms )\n",
    "    \n",
    "    # Compute volume of each object\n",
    "    unique, counts = np.unique( objects, return_counts=True )\n",
    "    # remove 0\n",
    "    unique = unique[1:]\n",
    "    counts = counts[1:]\n",
    "    \n",
    "    # create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # colum \"ID\"\n",
    "    df['ID'] = unique\n",
    "    \n",
    "    # column \"X\", \"Y\", \"Z\"\n",
    "    df['X'] = coms[ :, 2 ]\n",
    "    df['Y'] = coms[ :, 1 ]\n",
    "    df['Z'] = coms[ :, 0 ]\n",
    "    \n",
    "    # colum \"volume\"\n",
    "    df[\"volume\"] = counts\n",
    "    \n",
    "    # save as csv\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"csv/\" + basename[:-20] + \"_p30_all_639.csv\"\n",
    "    df.to_csv( filename, index=False, float_format='%.2f' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f51d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root diretory\n",
    "rootdir = \"I:\\Sup4\\hdf5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ecc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files which ends with 'probability'\n",
    "file_list = glob.glob( rootdir + \"*_Probabilities.h5\" )\n",
    "print( file_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b078464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all files\n",
    "for file in file_list:\n",
    "    ask_hdf5_size( file, dsetname=None )\n",
    "    raw = load_hdf5(file, multichannel=False)\n",
    "    print (file[50:-3])\n",
    "    calculate_prob_hdf5(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob_hdf5(file_list):\n",
    "    \n",
    "    # load probabiltiy image\n",
    "    prob = load_hdf5( file, \"exported_data\", multichannel=True )\n",
    "    print (prob.shape)\n",
    "    \n",
    "    ### Binarize probability image\n",
    "    thresh = 0.1 * 255\n",
    "    binary = ( prob > thresh )\n",
    "    print (\"Total volume of detected signals:\", binary.sum()*8.25*8.25*10)\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    # structure = ndi.generate_binary_structure( 3, 3 )\n",
    "    \n",
    "    # this defines \"connectivity\" between voxels\n",
    "    structure = np.array( [[[0,0,0],\n",
    "                            [0,0,0],\n",
    "                           [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]],\n",
    "                           [[0,0,0],\n",
    "                            [0,0,0],\n",
    "                            [0,0,0]]])\n",
    "        \n",
    "    # Label isolated objects\n",
    "    objects, num_objects = label( binary, structure )\n",
    "    print( \"Number of detected objects:\", objects.max() )\n",
    "        \n",
    "    # make binary into uint16\n",
    "    binary16 = (255*binary).astype( 'uint16' )\n",
    "    \n",
    "    # export as tiff\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"tiff/\" + basename[:-4] + \".tif\"\n",
    "    tifffile.imsave( filename, binary16 )\n",
    "    \n",
    "    ### Find center of mass\n",
    "    ids = np.arange( 1, num_objects+1 )\n",
    "    coms = ndi.center_of_mass( binary, objects, ids )\n",
    "    \n",
    "    # convert to numpy array\n",
    "    coms = np.array( coms )\n",
    "    \n",
    "    # Compute volume of each object\n",
    "    unique, counts = np.unique( objects, return_counts=True )\n",
    "    # remove 0\n",
    "    unique = unique[1:]\n",
    "    counts = counts[1:]\n",
    "    \n",
    "    # create empty dataframe\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # colum \"ID\"\n",
    "    df['ID'] = unique\n",
    "    \n",
    "    # column \"X\", \"Y\", \"Z\"\n",
    "    df['X'] = coms[ :, 2 ]\n",
    "    df['Y'] = coms[ :, 1 ]\n",
    "    df['Z'] = coms[ :, 0 ]\n",
    "    \n",
    "    # colum \"volume\"\n",
    "    df[\"volume\"] = counts\n",
    "    \n",
    "    # save as csv\n",
    "    basename = os.path.basename(file)\n",
    "    filename = rootdir[:-5] + \"csv/\" + basename[:-20] + \"_p10_all_639.csv\"\n",
    "    df.to_csv( filename, index=False, float_format='%.2f' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec30d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50383c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916299f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
