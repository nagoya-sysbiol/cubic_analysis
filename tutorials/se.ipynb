{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "67c75f06",
      "metadata": {
        "id": "67c75f06"
      },
      "source": [
        "# Signal extraction from CUBIC 3D image data using ilastik analysis\n",
        "\n",
        "This tutorial provides instructions for how to extract the signals from CUBIC 3d image data using ilastik. [The ilastik (interactive learning and segmentation toolkit)](https://www.ilastik.org/) is a Python library of interactive machine learning for (bio)image analysis. It implements leverage machine learning algorithms to easily segment, classify, track and count your cells or other experimental data developed by Berg et al., Nat Methods (2019). **Note. Original dataset has 454 images, but it's too large to work on Google Colab, therefore we picked up from 201th to 210th images to reduce the data size. h5 file is created after machine learning by ilastik software.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "2hIsFxb6ajUD"
      },
      "id": "2hIsFxb6ajUD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43fb5a35",
      "metadata": {
        "id": "43fb5a35"
      },
      "outputs": [],
      "source": [
        "import tifffile\n",
        "import glob\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "from scipy.ndimage import label\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.ndimage as ndi\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define required functions"
      ],
      "metadata": {
        "id": "_P2OGRGcdxpn"
      },
      "id": "_P2OGRGcdxpn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function to read tiff files"
      ],
      "metadata": {
        "id": "hPkvyVc4oo5z"
      },
      "id": "hPkvyVc4oo5z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff035c54",
      "metadata": {
        "id": "ff035c54"
      },
      "outputs": [],
      "source": [
        "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
        "    \"\"\"\n",
        "    load tiff sequence stored in the same directory\n",
        "    e.g. \n",
        "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
        "    \"\"\"\n",
        "\n",
        "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
        "    imlist.sort() # sort numerically\n",
        "    \n",
        "    if range is not None:\n",
        "        imlist = imlist[ range[0]:range[1]]\n",
        "        \n",
        "    #get image properties by reading the first image\n",
        "    im = tifffile.imread(imlist[0])\n",
        "    imsize_x = im.shape[1]\n",
        "    imsize_y = im.shape[0]\n",
        "    imsize_z = len( imlist )\n",
        "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
        "    imtype = im.dtype\n",
        "    \n",
        "    stack = np.zeros( imsize, dtype=imtype )\n",
        "    for (i, impath) in enumerate(imlist):\n",
        "        im = tifffile.imread( impath )\n",
        "        stack[i,:,:] = im\n",
        "        \n",
        "    return stack"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function to export as hdf5 file"
      ],
      "metadata": {
        "id": "xUVNO8PMoxBI"
      },
      "id": "xUVNO8PMoxBI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e96388ef",
      "metadata": {
        "id": "e96388ef"
      },
      "outputs": [],
      "source": [
        "def write_as_hdf5( stack, h5name, destname, \n",
        "                   chunks_enabled=True, chunksize=None,\n",
        "                   attributes=None ):\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
        "    \"\"\"\n",
        "    if chunks_enabled:\n",
        "        if chunksize is None:\n",
        "            chunks = True\n",
        "        else:\n",
        "            chunks = chunksize\n",
        "    else:\n",
        "        chunks = None\n",
        "        \n",
        "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
        "        data = hf.create_dataset (destname,\n",
        "                                  chunks=chunks,\n",
        "                                  data=stack )\n",
        "        if attributes is not None:\n",
        "            for key, value in attributes.items():\n",
        "                data.attrs[key] = value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function to return the size of a hdf5 file"
      ],
      "metadata": {
        "id": "QSd4QQbFo3p5"
      },
      "id": "QSd4QQbFo3p5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2d8c89",
      "metadata": {
        "id": "9e2d8c89"
      },
      "outputs": [],
      "source": [
        "def ask_hdf5_size( h5name, dsetname=None ):\n",
        "    \n",
        "    # obtain file handle\n",
        "    hf = h5py.File( h5name, 'r' )\n",
        "    \n",
        "    if dsetname is None:\n",
        "        # get the name of the 0th dataset\n",
        "        dsetname = list( hf.keys() )[0]\n",
        "        dset = hf[ dsetname ]\n",
        "    else:\n",
        "        # get dataset\n",
        "        dset = hf[ dsetname ]\n",
        "    \n",
        "    # print size\n",
        "    print( \"Data set size:\", dset.shape )\n",
        "    \n",
        "    # close handle\n",
        "    hf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function to read hdf5 files"
      ],
      "metadata": {
        "id": "5YyDK48spivl"
      },
      "id": "5YyDK48spivl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96b8d39e",
      "metadata": {
        "id": "96b8d39e"
      },
      "outputs": [],
      "source": [
        "def load_hdf5( h5name, dsetname=None, multichannel=True ):\n",
        "    \n",
        "    # obtain file handle\n",
        "    hf = h5py.File( h5name, 'r' )\n",
        "    \n",
        "    if dsetname is None:\n",
        "        # get the name of the 0th dataset\n",
        "        dsetname = list( hf.keys() )[0]\n",
        "        dset = hf[ dsetname ]\n",
        "    else:\n",
        "        # get dataset\n",
        "        dset = hf[ dsetname ]\n",
        "    \n",
        "    if multichannel:\n",
        "        # load data as numpy array\n",
        "        data = dset[ :, :, :, 0] # 0th channel = cells\n",
        "        #data = dset[ :, :, :, 0] # 0th channel = cells\n",
        "    else:\n",
        "        data = dset[ :, :, :] # 0th channel = cells\n",
        "        #data = dset[ :, :, :] # 0th channel = cells\n",
        "\n",
        "    # close handle\n",
        "    hf.close()\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function to binarize probability images by thresholding"
      ],
      "metadata": {
        "id": "whNH_DPhpuYE"
      },
      "id": "whNH_DPhpuYE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d50b613",
      "metadata": {
        "id": "6d50b613"
      },
      "outputs": [],
      "source": [
        "def calculate_prob_hdf5(file_list, threshold):\n",
        "    \n",
        "    # load probabiltiy image\n",
        "    prob = load_hdf5( file, \"expmat\", multichannel=True )\n",
        "    print (prob.shape)\n",
        "    \n",
        "    ### Binarize probability image\n",
        "    thresh = threshold * 255\n",
        "    binary = ( prob > thresh )\n",
        "    print (\"Total volume of detected signals:\", binary.sum()*8.25*8.25*10)\n",
        "    \n",
        "    # this defines \"connectivity\" between voxels\n",
        "    # structure = ndi.generate_binary_structure( 3, 3 )\n",
        "    \n",
        "    # this defines \"connectivity\" between voxels\n",
        "    structure = np.array( [[[0,0,0],\n",
        "                            [0,0,0],\n",
        "                           [0,0,0]],\n",
        "                           [[0,0,0],\n",
        "                            [0,0,0],\n",
        "                            [0,0,0]],\n",
        "                           [[0,0,0],\n",
        "                            [0,0,0],\n",
        "                            [0,0,0]]])\n",
        "        \n",
        "    # Label isolated objects\n",
        "    objects, num_objects = label( binary, structure )\n",
        "    print( \"Number of detected objects:\", objects.max() )\n",
        "        \n",
        "    # make binary into uint16\n",
        "    binary16 = (255*binary).astype( 'uint16' )\n",
        "    \n",
        "    # export as tiff\n",
        "    basename = os.path.basename(file)\n",
        "    tiffdir = rootdir + \"tiff\"\n",
        "    if not os.path.exists(tiffdir):\n",
        "      os.mkdir(tiffdir)\n",
        "    filename = rootdir + \"tiff/\" + basename[:-26] + f\"_p{int(threshold*100)}_all_639.tiff\"\n",
        "    tifffile.imsave( filename, binary16 )\n",
        "    \n",
        "    ### Find center of mass\n",
        "    ids = np.arange( 1, num_objects+1 )\n",
        "    coms = ndi.center_of_mass( binary, objects, ids )\n",
        "    \n",
        "    # convert to numpy array\n",
        "    coms = np.array( coms )\n",
        "    \n",
        "    # Compute volume of each object\n",
        "    unique, counts = np.unique( objects, return_counts=True )\n",
        "    # remove 0\n",
        "    unique = unique[1:]\n",
        "    counts = counts[1:]\n",
        "    \n",
        "    # create empty dataframe\n",
        "    df = pd.DataFrame()\n",
        "    \n",
        "    # colum \"ID\"\n",
        "    df['ID'] = unique\n",
        "    \n",
        "    # column \"X\", \"Y\", \"Z\"\n",
        "    df['X'] = coms[ :, 2 ]\n",
        "    df['Y'] = coms[ :, 1 ]\n",
        "    df['Z'] = coms[ :, 0 ]\n",
        "    \n",
        "    # colum \"volume\"\n",
        "    df[\"volume\"] = counts\n",
        "    \n",
        "    # save as csv\n",
        "    basename = os.path.basename(file)\n",
        "    csvdir = rootdir + \"csv\"\n",
        "    if not os.path.exists(csvdir):\n",
        "      os.mkdir(csvdir)\n",
        "    filename = rootdir + \"csv/\" + basename[:-26] + f\"_p{int(threshold*100)}_all_639.csv\"\n",
        "    df.to_csv( filename, index=False, float_format='%.2f' )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Download"
      ],
      "metadata": {
        "id": "lXPdCQ9qXBRX"
      },
      "id": "lXPdCQ9qXBRX"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
        "!wget https://www.dropbox.com/s/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
        "\n",
        "! unzip 190604_#144_lung_raw_tiff_10slices.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqbh8CUi2627",
        "outputId": "5db8452f-8dd0-4cc5-a5df-d8440543ad8a"
      },
      "id": "Dqbh8CUi2627",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-12 01:08:18--  https://www.dropbox.com/s/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip [following]\n",
            "--2022-08-12 01:08:19--  https://www.dropbox.com/s/raw/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5263c131ee2dab783539ded818.dl.dropboxusercontent.com/cd/0/inline/Bq39375uK6TNyJt1cXp7U0RKPCJecdKsVl4_byHSDdT9IPYFnV1c32UjyHeApzYMEqZzEukShP2EFChE2HKCfY0f9-nnpcUlA_6iCRr-jURIugeEo0f5Dg0cxIzxR2ZzZ2IYgxQtG5ZwXLJ2WbfmEnBCoqeaj8OtORr1plK-hAwLTQ/file# [following]\n",
            "--2022-08-12 01:08:19--  https://uc5263c131ee2dab783539ded818.dl.dropboxusercontent.com/cd/0/inline/Bq39375uK6TNyJt1cXp7U0RKPCJecdKsVl4_byHSDdT9IPYFnV1c32UjyHeApzYMEqZzEukShP2EFChE2HKCfY0f9-nnpcUlA_6iCRr-jURIugeEo0f5Dg0cxIzxR2ZzZ2IYgxQtG5ZwXLJ2WbfmEnBCoqeaj8OtORr1plK-hAwLTQ/file\n",
            "Resolving uc5263c131ee2dab783539ded818.dl.dropboxusercontent.com (uc5263c131ee2dab783539ded818.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to uc5263c131ee2dab783539ded818.dl.dropboxusercontent.com (uc5263c131ee2dab783539ded818.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bq0Z-jeLSLIGu3gwFtatv_aPZon59izIeToPGVlc0dSBGBQhVR2BTxxrLt0IfTw5o3dGKyMzQvm2OPHvy9YeWLMN8_TxyozenSLH_eVrRDhvxnLFKdWEpldgybDLo5CrnnkbNAvOCd67I_aDhGTsZ1DJcHj7wPj99gjoJvs_ihkXuA6IkJqa0KFazpdXaoRwTCeUKLOY3OyYTjzJpAobJgt-iFsm2gJa6UKEA5M-uBTXlxFV4fRnkKsSAlqBdTk7maL6LiOOKRlH-OHf30VxhKqdBdk2KNhIDhtHQ2Rwx50ftySsW64aUUhgvoheBQNnVCq0iU5BbCciqYE3cFvcuhuQLMf6HVRGY1KSLqRcGu506e7KWkQuaI42entvpzqNB5BVjS8v6GBGULkcfXimCfKAXBu9K_3cS1mqd2rboOgsBA/file [following]\n",
            "--2022-08-12 01:08:19--  https://uc5263c131ee2dab783539ded818.dl.dropboxusercontent.com/cd/0/inline2/Bq0Z-jeLSLIGu3gwFtatv_aPZon59izIeToPGVlc0dSBGBQhVR2BTxxrLt0IfTw5o3dGKyMzQvm2OPHvy9YeWLMN8_TxyozenSLH_eVrRDhvxnLFKdWEpldgybDLo5CrnnkbNAvOCd67I_aDhGTsZ1DJcHj7wPj99gjoJvs_ihkXuA6IkJqa0KFazpdXaoRwTCeUKLOY3OyYTjzJpAobJgt-iFsm2gJa6UKEA5M-uBTXlxFV4fRnkKsSAlqBdTk7maL6LiOOKRlH-OHf30VxhKqdBdk2KNhIDhtHQ2Rwx50ftySsW64aUUhgvoheBQNnVCq0iU5BbCciqYE3cFvcuhuQLMf6HVRGY1KSLqRcGu506e7KWkQuaI42entvpzqNB5BVjS8v6GBGULkcfXimCfKAXBu9K_3cS1mqd2rboOgsBA/file\n",
            "Reusing existing connection to uc5263c131ee2dab783539ded818.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49084620 (47M) [application/zip]\n",
            "Saving to: ‘190604_#144_lung_raw_tiff_10slices.zip’\n",
            "\n",
            "190604_#144_lung_ra 100%[===================>]  46.81M  61.7MB/s    in 0.8s    \n",
            "\n",
            "2022-08-12 01:08:21 (61.7 MB/s) - ‘190604_#144_lung_raw_tiff_10slices.zip’ saved [49084620/49084620]\n",
            "\n",
            "--2022-08-12 01:08:21--  https://www.dropbox.com/s/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5 [following]\n",
            "--2022-08-12 01:08:21--  https://www.dropbox.com/s/raw/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc960908e58272bf682f4c67d967.dl.dropboxusercontent.com/cd/0/inline/Bq1tgv7FISpmPaqAoX7vRGTtYnk5YLbFWJ0DNcW4qjoFbYLcF3o3-dK4WNvHIt83Tc19CxJLZWXJMkZIZ_sR0QyrKz0U12gK1KJhbZ8pZ5vVEPMwh6TP9A0Q97v25IBTzJcmyhvUNIksf38ZPMRnhMB6GEuUY7qdABDI-kGKoOxDHg/file# [following]\n",
            "--2022-08-12 01:08:22--  https://uc960908e58272bf682f4c67d967.dl.dropboxusercontent.com/cd/0/inline/Bq1tgv7FISpmPaqAoX7vRGTtYnk5YLbFWJ0DNcW4qjoFbYLcF3o3-dK4WNvHIt83Tc19CxJLZWXJMkZIZ_sR0QyrKz0U12gK1KJhbZ8pZ5vVEPMwh6TP9A0Q97v25IBTzJcmyhvUNIksf38ZPMRnhMB6GEuUY7qdABDI-kGKoOxDHg/file\n",
            "Resolving uc960908e58272bf682f4c67d967.dl.dropboxusercontent.com (uc960908e58272bf682f4c67d967.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6023:15::a27d:430f\n",
            "Connecting to uc960908e58272bf682f4c67d967.dl.dropboxusercontent.com (uc960908e58272bf682f4c67d967.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110594048 (105M) [text/plain]\n",
            "Saving to: ‘190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5’\n",
            "\n",
            "190604_P_#144_lung_ 100%[===================>] 105.47M  38.4MB/s    in 2.8s    \n",
            "\n",
            "2022-08-12 01:08:26 (38.4 MB/s) - ‘190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5’ saved [110594048/110594048]\n",
            "\n",
            "Archive:  190604_#144_lung_raw_tiff_10slices.zip\n",
            "   creating: 190604_#144_lung_raw_tiff_10slices/\n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00201]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00202]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00203]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00204]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00205]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00206]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00207]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00208]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00209]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00210]_L[3].tiff  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose Tiff file folder"
      ],
      "metadata": {
        "id": "uHBVW4zDfGqH"
      },
      "id": "uHBVW4zDfGqH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6ce507",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b6ce507",
        "outputId": "5ff2a4ac-abb4-4a6c-c020-ce48f8363478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/190604_#144_lung_raw_tiff_10slices\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"/content/190604_#144_lung_raw_tiff_10slices\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Tiff file"
      ],
      "metadata": {
        "id": "ZSHwdwMqfZLB"
      },
      "id": "ZSHwdwMqfZLB"
    },
    {
      "cell_type": "code",
      "source": [
        "imgdir = \"/content/190604_#144_lung_raw_tiff_10slices/\"\n",
        "img = load_tiff_sequence( imgdir, imgtype='tiff')\n",
        "\n",
        "print(img.shape)"
      ],
      "metadata": {
        "id": "NTilBVcffeFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6744b37-ffe3-425f-d276-1e3d042ec565"
      },
      "id": "NTilBVcffeFP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 2160, 2560)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save as hdf5"
      ],
      "metadata": {
        "id": "iKDg0JNnfsGB"
      },
      "id": "iKDg0JNnfsGB"
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/190604_P_#144_lung_ctrl_x125_639_10slices.hdf5\"\n",
        "dname = \"content\"\n",
        "\n",
        "write_as_hdf5( img, filename, dname, chunks_enabled=True, chunksize=(10,100,100) )"
      ],
      "metadata": {
        "id": "48sOrshifsYx"
      },
      "id": "48sOrshifsYx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c2660bb2",
      "metadata": {
        "id": "c2660bb2"
      },
      "source": [
        "## Binarization of probability images by thresholding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87aa710e",
      "metadata": {
        "id": "87aa710e"
      },
      "outputs": [],
      "source": [
        "h5name = \"/content/190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5\"\n",
        "hf = h5py.File( h5name, \"r\" )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = hf[\"expmat\"]\n",
        "print (data.shape)\n",
        "l1_prob = data[:,:,:,0] # probability of label1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siTkiqKVEX9v",
        "outputId": "d283b5e3-f09b-44fe-fc3a-8aab329163a8"
      },
      "id": "siTkiqKVEX9v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 2160, 2560, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds = [24, 50, 75, 101, 126, 152, 178, 203, 229]\n",
        "for thresh, percent, idx in zip(thresholds, range(10, 100, 10), list(string.ascii_uppercase)):\n",
        "  # maks a binary mask\n",
        "  binary = (l1_prob > thresh)\n",
        "  print (binary.sum()*8.25*8.25*10)\n",
        "  # make binary into uint8\n",
        "  binary = (255*binary).astype('uint16')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDrc4ay2YqQQ",
        "outputId": "10bbb723-2c1d-4a89-bb07-0d0f1cfa53a3"
      },
      "id": "pDrc4ay2YqQQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165508942.5\n",
            "133056061.875\n",
            "119030422.5\n",
            "103172540.625\n",
            "85788016.875\n",
            "72314364.375\n",
            "59966465.625\n",
            "49488243.75\n",
            "33390781.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define root diretory"
      ],
      "metadata": {
        "id": "_vXRPYa5mn-H"
      },
      "id": "_vXRPYa5mn-H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2b2c2a",
      "metadata": {
        "id": "bc2b2c2a"
      },
      "outputs": [],
      "source": [
        "rootdir = \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get files which ends with 'probability'"
      ],
      "metadata": {
        "id": "L-LJU1HYmgNq"
      },
      "id": "L-LJU1HYmgNq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9f38c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c9f38c6",
        "outputId": "b99bf91b-c46d-42da-eb12-55cda023f299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5']\n"
          ]
        }
      ],
      "source": [
        "file_list = glob.glob( rootdir + \"*_Probabilities_10slices.h5\" )\n",
        "print( file_list )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = file_list[0]\n",
        "prob = load_hdf5( file, \"expmat\", multichannel=False )"
      ],
      "metadata": {
        "id": "-lGLhY7DGom6"
      },
      "id": "-lGLhY7DGom6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop through all files and thresholds"
      ],
      "metadata": {
        "id": "dY3ABS59mX4B"
      },
      "id": "dY3ABS59mX4B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3176aea9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3176aea9",
        "outputId": "991dddc9-9a95-4d02-d839-2f7ddd423d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 160266768.75\n",
            "Number of detected objects: 235470\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 117569120.625\n",
            "Number of detected objects: 172737\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 84283835.625\n",
            "Number of detected objects: 123833\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 59966465.625\n",
            "Number of detected objects: 88105\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 33390781.875\n",
            "Number of detected objects: 49059\n"
          ]
        }
      ],
      "source": [
        "thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "for file in file_list:\n",
        "    for thresh in thresholds:\n",
        "      ask_hdf5_size( file, dsetname=None )\n",
        "      raw = load_hdf5(file, multichannel=False)\n",
        "      print (file.rsplit(\"/\")[-1][:-3])\n",
        "      calculate_prob_hdf5(file, thresh)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "se.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
