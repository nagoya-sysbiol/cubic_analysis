{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "67c75f06",
      "metadata": {
        "id": "67c75f06"
      },
      "source": [
        "# Convert tiff files to hdf5 file (for ilastik analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "43fb5a35",
      "metadata": {
        "id": "43fb5a35"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "\n",
        "import tifffile\n",
        "import glob\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff035c54",
      "metadata": {
        "id": "ff035c54"
      },
      "outputs": [],
      "source": [
        "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
        "    \"\"\"\n",
        "    load tiff sequence stored in the same directory\n",
        "    e.g. \n",
        "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
        "    \"\"\"\n",
        "\n",
        "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
        "    imlist.sort() # sort numerically\n",
        "    \n",
        "    if range is not None:\n",
        "        imlist = imlist[ range[0]:range[1]]\n",
        "        \n",
        "    #get image properties by reading the first image\n",
        "    im = tifffile.imread(imlist[0])\n",
        "    imsize_x = im.shape[1]\n",
        "    imsize_y = im.shape[0]\n",
        "    imsize_z = len( imlist )\n",
        "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
        "    imtype = im.dtype\n",
        "    \n",
        "    stack = np.zeros( imsize, dtype=imtype )\n",
        "    for (i, impath) in enumerate(imlist):\n",
        "        im = tifffile.imread( impath )\n",
        "        stack[i,:,:] = im\n",
        "        \n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e96388ef",
      "metadata": {
        "id": "e96388ef"
      },
      "outputs": [],
      "source": [
        "def write_as_hdf5( stack, h5name, destname, \n",
        "                   chunks_enabled=True, chunksize=None,\n",
        "                   attributes=None ):\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
        "    \"\"\"\n",
        "    if chunks_enabled:\n",
        "        if chunksize is None:\n",
        "            chunks = True\n",
        "        else:\n",
        "            chunks = chunksize\n",
        "    else:\n",
        "        chunks = None\n",
        "        \n",
        "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
        "        data = hf.create_dataset (destname,\n",
        "                                  chunks=chunks,\n",
        "                                  data=stack )\n",
        "        if attributes is not None:\n",
        "            for key, value in attributes.items():\n",
        "                data.attrs[key] = value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗\n",
        "Original dataset has 454 images, but it's too large to work on Google Colab, therefore we picked up 201~210th images to reduce the data size. h5 file is created after machine learning by ilastik software."
      ],
      "metadata": {
        "id": "lXPdCQ9qXBRX"
      },
      "id": "lXPdCQ9qXBRX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Download\n",
        "!wget https://www.dropbox.com/s/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
        "!wget https://www.dropbox.com/s/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
        "\n",
        "! unzip 190604_#144_lung_raw_tiff_10slices.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqbh8CUi2627",
        "outputId": "fda860dc-37ad-45eb-cdf7-d848f9669ba7"
      },
      "id": "Dqbh8CUi2627",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-11 04:48:39--  https://www.dropbox.com/s/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip [following]\n",
            "--2022-08-11 04:48:40--  https://www.dropbox.com/s/raw/z0cmicvosckuqqr/190604_%23144_lung_raw_tiff_10slices.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucbd8fcd6af39d0f96ea48a6ce1d.dl.dropboxusercontent.com/cd/0/inline/Bqzn3rCuRW6GBeQIVrmmqJAuN_FfjU81zjBXYYrGxZXQGK7IGceYaQzKpDZoqBT6CBk5O0okPpC1sK7bHl4-CVn2bVOtlkmhJUZrCvaHPhbUroWVsm96AsOEN59vVzoS_xyIffv0NNeODO5zItH9jlEMZH4JyBUg7rO-SmvPIOHhZg/file# [following]\n",
            "--2022-08-11 04:48:40--  https://ucbd8fcd6af39d0f96ea48a6ce1d.dl.dropboxusercontent.com/cd/0/inline/Bqzn3rCuRW6GBeQIVrmmqJAuN_FfjU81zjBXYYrGxZXQGK7IGceYaQzKpDZoqBT6CBk5O0okPpC1sK7bHl4-CVn2bVOtlkmhJUZrCvaHPhbUroWVsm96AsOEN59vVzoS_xyIffv0NNeODO5zItH9jlEMZH4JyBUg7rO-SmvPIOHhZg/file\n",
            "Resolving ucbd8fcd6af39d0f96ea48a6ce1d.dl.dropboxusercontent.com (ucbd8fcd6af39d0f96ea48a6ce1d.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to ucbd8fcd6af39d0f96ea48a6ce1d.dl.dropboxusercontent.com (ucbd8fcd6af39d0f96ea48a6ce1d.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bqxx3EpJo5MQqNjc0kP0IZowS5x8sBqNUY2eh3wzyL2LjVSZNYh2IePXEktawldG1l0mjDjW9PGKnLUbLhayYObRHg_ZuyuJ4WbfVgbDBUNUGKrRBdefgeavfvnuWwQKhMPcf8FLnTKweyhzBlfdERe70Gs74YDLlLnN8dHi9Fvs77e3BghWEM_o0VArl73U1nxj6DVVHJ0G5Tdyik4eeIm4quA0KM-dA8SoaXEdHLB3a3wNV-FUgPZXZJZ4C_Ld9UkJDXjqDzzJVRn9Qlq01UnjUYeLwDbD7mvo3ypoCERN7rmTV5I7I2sNIWNjdavznaMmZGGKGiS2Y2S8pICfTeFEJZsr4Khx8Mk4nqX25k5e8Ahr8wn48JA1gcVzM_GOo9c_tKuBzYIFf2o6m09xZfinfTyS2vQDI530olI3L-d53A/file [following]\n",
            "--2022-08-11 04:48:40--  https://ucbd8fcd6af39d0f96ea48a6ce1d.dl.dropboxusercontent.com/cd/0/inline2/Bqxx3EpJo5MQqNjc0kP0IZowS5x8sBqNUY2eh3wzyL2LjVSZNYh2IePXEktawldG1l0mjDjW9PGKnLUbLhayYObRHg_ZuyuJ4WbfVgbDBUNUGKrRBdefgeavfvnuWwQKhMPcf8FLnTKweyhzBlfdERe70Gs74YDLlLnN8dHi9Fvs77e3BghWEM_o0VArl73U1nxj6DVVHJ0G5Tdyik4eeIm4quA0KM-dA8SoaXEdHLB3a3wNV-FUgPZXZJZ4C_Ld9UkJDXjqDzzJVRn9Qlq01UnjUYeLwDbD7mvo3ypoCERN7rmTV5I7I2sNIWNjdavznaMmZGGKGiS2Y2S8pICfTeFEJZsr4Khx8Mk4nqX25k5e8Ahr8wn48JA1gcVzM_GOo9c_tKuBzYIFf2o6m09xZfinfTyS2vQDI530olI3L-d53A/file\n",
            "Reusing existing connection to ucbd8fcd6af39d0f96ea48a6ce1d.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49084620 (47M) [application/zip]\n",
            "Saving to: ‘190604_#144_lung_raw_tiff_10slices.zip’\n",
            "\n",
            "190604_#144_lung_ra 100%[===================>]  46.81M   183MB/s    in 0.3s    \n",
            "\n",
            "2022-08-11 04:48:41 (183 MB/s) - ‘190604_#144_lung_raw_tiff_10slices.zip’ saved [49084620/49084620]\n",
            "\n",
            "--2022-08-11 04:48:41--  https://www.dropbox.com/s/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5 [following]\n",
            "--2022-08-11 04:48:41--  https://www.dropbox.com/s/raw/al85vb3bxwl250g/190604_P_%23144_lung_ctrl_x125_639_Probabilities_10slices.h5\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc525d85088d742cc438e16939f7.dl.dropboxusercontent.com/cd/0/inline/BqwJZjCGX9zFB3Ulbjb2zusELU2XS4h7HmJN18_0m8w1BeJZE64Q2Zkd2zNPWZQ46emiEL8pfWhNdKBOHbNt2_5ygSn2mxMGF9vE4ygiZ6umJ4_EQoq71jECdGRpr-Xlg15XbeClmnxEqQ29U2WryV9lNcxDFQwjIRwk2UEqDoR-jw/file# [following]\n",
            "--2022-08-11 04:48:42--  https://uc525d85088d742cc438e16939f7.dl.dropboxusercontent.com/cd/0/inline/BqwJZjCGX9zFB3Ulbjb2zusELU2XS4h7HmJN18_0m8w1BeJZE64Q2Zkd2zNPWZQ46emiEL8pfWhNdKBOHbNt2_5ygSn2mxMGF9vE4ygiZ6umJ4_EQoq71jECdGRpr-Xlg15XbeClmnxEqQ29U2WryV9lNcxDFQwjIRwk2UEqDoR-jw/file\n",
            "Resolving uc525d85088d742cc438e16939f7.dl.dropboxusercontent.com (uc525d85088d742cc438e16939f7.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc525d85088d742cc438e16939f7.dl.dropboxusercontent.com (uc525d85088d742cc438e16939f7.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110594048 (105M) [text/plain]\n",
            "Saving to: ‘190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5’\n",
            "\n",
            "190604_P_#144_lung_ 100%[===================>] 105.47M   108MB/s    in 1.0s    \n",
            "\n",
            "2022-08-11 04:48:44 (108 MB/s) - ‘190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5’ saved [110594048/110594048]\n",
            "\n",
            "Archive:  190604_#144_lung_raw_tiff_10slices.zip\n",
            "   creating: 190604_#144_lung_raw_tiff_10slices/\n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00201]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00202]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00203]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00204]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00205]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00206]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00207]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00208]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00209]_L[3].tiff  \n",
            "  inflating: 190604_#144_lung_raw_tiff_10slices/Merge Image_Z[00210]_L[3].tiff  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7b6ce507",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b6ce507",
        "outputId": "87966300-1eed-4c55-e32b-f748da583a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/190604_#144_lung_raw_tiff_10slices\n",
            "(10, 2160, 2560)\n"
          ]
        }
      ],
      "source": [
        "#Choose Tiff file folder\n",
        "os.chdir(\"/content/190604_#144_lung_raw_tiff_10slices\")\n",
        "print(os.getcwd())\n",
        "\n",
        "#Read Tiff file\n",
        "imgdir = \"/content/190604_#144_lung_raw_tiff_10slices//\"\n",
        "img = load_tiff_sequence( imgdir, imgtype='tiff')\n",
        "\n",
        "print(img.shape)\n",
        "\n",
        "#Save as hdf5\n",
        "filename = \"/content/190604_P_#144_lung_ctrl_x125_639_10slices.hdf5\"\n",
        "dname = \"content\"\n",
        "\n",
        "write_as_hdf5( img, filename, dname, chunks_enabled=True, chunksize=(10,100,100) )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2660bb2",
      "metadata": {
        "id": "c2660bb2"
      },
      "source": [
        "# probability threshold (after ilastik analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "350243f0",
      "metadata": {
        "id": "350243f0"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import tifffile\n",
        "import glob\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f62c5f01",
      "metadata": {
        "id": "f62c5f01"
      },
      "outputs": [],
      "source": [
        "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
        "    \"\"\"\n",
        "    load tiff sequence stored in the same directory\n",
        "    e.g. \n",
        "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
        "    \"\"\"\n",
        "\n",
        "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
        "    imlist.sort() # sort numerically\n",
        "    \n",
        "    if range is not None:\n",
        "        imlist = imlist[ range[0]:range[1]]\n",
        "        \n",
        "    #get image properties by reading the first image\n",
        "    im = tifffile.imread(imlist[0])\n",
        "    imsize_x = im.shape[1]\n",
        "    imsize_y = im.shape[0]\n",
        "    imsize_z = len( imlist )\n",
        "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
        "    imtype = im.dtype\n",
        "    \n",
        "    stack = np.zeros( imsize, dtype=imtype )\n",
        "    for (i, impath) in enumerate(imlist):\n",
        "        im = tifffile.imread( impath )\n",
        "        stack[i,:,:] = im\n",
        "        \n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "38ab0e43",
      "metadata": {
        "id": "38ab0e43"
      },
      "outputs": [],
      "source": [
        "def write_as_hdf5( stack, h5name, destname, \n",
        "                   chunks_enabled=True, chunksize=None,\n",
        "                   attributes=None ):\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
        "    \"\"\"\n",
        "    if chunks_enabled:\n",
        "        if chunksize is None:\n",
        "            chunks = True\n",
        "        else:\n",
        "            chunks = chunksize\n",
        "    else:\n",
        "        chunks = None\n",
        "        \n",
        "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
        "        data = hf.create_dataset (destname,\n",
        "                                  chunks=chunks,\n",
        "                                  data=stack )\n",
        "        if attributes is not None:\n",
        "            for key, value in attributes.items():\n",
        "                data.attrs[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "87aa710e",
      "metadata": {
        "id": "87aa710e"
      },
      "outputs": [],
      "source": [
        "h5name = \"/content/190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5\"\n",
        "hf = h5py.File( h5name, \"r\" )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = hf[\"expmat\"]\n",
        "print (data.shape)\n",
        "l1_prob = data[:,:,:,0] # probability of label1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siTkiqKVEX9v",
        "outputId": "e922bf03-c5bd-4a54-c502-976b5e6d50b9"
      },
      "id": "siTkiqKVEX9v",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 2160, 2560, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds = [24, 50, 75, 101, 126, 152, 178, 203, 229]\n",
        "for thresh, percent, idx in zip(thresholds, range(10, 100, 10), list(string.ascii_uppercase)):\n",
        "  # maks a binary mask\n",
        "  binary = (l1_prob > thresh)\n",
        "  print (binary.sum()*8.25*8.25*10)\n",
        "  # make binary into uint8\n",
        "  binary = (255*binary).astype( 'uint16' )\n",
        "  # export as tiff\n",
        "  filename = f\"/content/190604_P_#144_lung_ctrl_x125_639_bin{percent}{idx}.tiff\"\n",
        "  tifffile.imsave( filename, binary )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDrc4ay2YqQQ",
        "outputId": "a6e0f739-0634-4c98-e5d6-2860319b0ae8"
      },
      "id": "pDrc4ay2YqQQ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165508942.5\n",
            "133056061.875\n",
            "119030422.5\n",
            "103172540.625\n",
            "85788016.875\n",
            "72314364.375\n",
            "59966465.625\n",
            "49488243.75\n",
            "33390781.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b7d394e",
      "metadata": {
        "id": "0b7d394e"
      },
      "source": [
        "# Count all signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "61d34cb8",
      "metadata": {
        "id": "61d34cb8"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "# from skimage.external import tifffile # Error, use tifffile library\n",
        "import tifffile\n",
        "from scipy.ndimage import label\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.ndimage as ndi\n",
        "import glob\n",
        "import h5py\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "eb8e45e2",
      "metadata": {
        "id": "eb8e45e2"
      },
      "outputs": [],
      "source": [
        "def load_tiff_sequence ( imdir, imgtype='tiff', range=None ):\n",
        "    \"\"\"\n",
        "    load tiff sequence stored in the same directory\n",
        "    e.g. \n",
        "    vol = load_tiff_sequence (imgdir, '.png', range=[])\n",
        "    \"\"\"\n",
        "\n",
        "    imlist = glob.glob( imdir + '*.' + imgtype )\n",
        "    imlist.sort() # sort numerically\n",
        "    \n",
        "    if range is not None:\n",
        "        imlist = imlist[ range[0]:range[1]]\n",
        "        \n",
        "    #get image properties by reading the first image\n",
        "    im = tifffile.imread(imlist[0])\n",
        "    imsize_x = im.shape[1]\n",
        "    imsize_y = im.shape[0]\n",
        "    imsize_z = len( imlist )\n",
        "    imsize = ( imsize_z, imsize_y, imsize_x )\n",
        "    imtype = im.dtype\n",
        "    \n",
        "    stack = np.zeros( imsize, dtype=imtype )\n",
        "    for (i, impath) in enumerate(imlist):\n",
        "        im = tifffile.imread( impath )\n",
        "        stack[i,:,:] = im\n",
        "        \n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "22e92c5b",
      "metadata": {
        "id": "22e92c5b"
      },
      "outputs": [],
      "source": [
        "def write_as_hdf5( stack, h5name, destname, \n",
        "                   chunks_enabled=True, chunksize=None,\n",
        "                   attributes=None ):\n",
        "    \"\"\"\n",
        "    e.g.\n",
        "    write_as_hdf5(vol, 'test.hdf5', 'resolution_0', True, (100,100,100))\n",
        "    \"\"\"\n",
        "    if chunks_enabled:\n",
        "        if chunksize is None:\n",
        "            chunks = True\n",
        "        else:\n",
        "            chunks = chunksize\n",
        "    else:\n",
        "        chunks = None\n",
        "        \n",
        "    with h5py.File( h5name, 'w', driver='stdio' ) as hf:\n",
        "        data = hf.create_dataset (destname,\n",
        "                                  chunks=chunks,\n",
        "                                  data=stack )\n",
        "        if attributes is not None:\n",
        "            for key, value in attributes.items():\n",
        "                data.attrs[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9e2d8c89",
      "metadata": {
        "id": "9e2d8c89"
      },
      "outputs": [],
      "source": [
        "def ask_hdf5_size( h5name, dsetname=None ):\n",
        "    \n",
        "    # obtain file handle\n",
        "    hf = h5py.File( h5name, 'r' )\n",
        "    \n",
        "    if dsetname is None:\n",
        "        # get the name of the 0th dataset\n",
        "        dsetname = list( hf.keys() )[0]\n",
        "        dset = hf[ dsetname ]\n",
        "    else:\n",
        "        # get dataset\n",
        "        dset = hf[ dsetname ]\n",
        "    \n",
        "    # print size\n",
        "    print( \"Data set size:\", dset.shape )\n",
        "    \n",
        "    # close handle\n",
        "    hf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "96b8d39e",
      "metadata": {
        "id": "96b8d39e"
      },
      "outputs": [],
      "source": [
        "def load_hdf5( h5name, dsetname=None, multichannel=True ):\n",
        "    \n",
        "    # obtain file handle\n",
        "    hf = h5py.File( h5name, 'r' )\n",
        "    \n",
        "    if dsetname is None:\n",
        "        # get the name of the 0th dataset\n",
        "        dsetname = list( hf.keys() )[0]\n",
        "        dset = hf[ dsetname ]\n",
        "    else:\n",
        "        # get dataset\n",
        "        dset = hf[ dsetname ]\n",
        "    \n",
        "    if multichannel:\n",
        "        # load data as numpy array\n",
        "        data = dset[ :, :, :, 0] # 0th channel = cells\n",
        "        #data = dset[ :, :, :, 0] # 0th channel = cells\n",
        "    else:\n",
        "        data = dset[ :, :, :] # 0th channel = cells\n",
        "        #data = dset[ :, :, :] # 0th channel = cells\n",
        "\n",
        "    # close handle\n",
        "    hf.close()\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6d50b613",
      "metadata": {
        "id": "6d50b613"
      },
      "outputs": [],
      "source": [
        "def calculate_prob_hdf5(file_list, threshold):\n",
        "    \n",
        "    # load probabiltiy image\n",
        "    prob = load_hdf5( file, \"expmat\", multichannel=True )\n",
        "    print (prob.shape)\n",
        "    \n",
        "    ### Binarize probability image\n",
        "    thresh = threshold * 255\n",
        "    binary = ( prob > thresh )\n",
        "    print (\"Total volume of detected signals:\", binary.sum()*8.25*8.25*10)\n",
        "    \n",
        "    # this defines \"connectivity\" between voxels\n",
        "    # structure = ndi.generate_binary_structure( 3, 3 )\n",
        "    \n",
        "    # this defines \"connectivity\" between voxels\n",
        "    structure = np.array( [[[0,0,0],\n",
        "                            [0,0,0],\n",
        "                           [0,0,0]],\n",
        "                           [[0,0,0],\n",
        "                            [0,0,0],\n",
        "                            [0,0,0]],\n",
        "                           [[0,0,0],\n",
        "                            [0,0,0],\n",
        "                            [0,0,0]]])\n",
        "        \n",
        "    # Label isolated objects\n",
        "    objects, num_objects = label( binary, structure )\n",
        "    print( \"Number of detected objects:\", objects.max() )\n",
        "        \n",
        "    # make binary into uint16\n",
        "    binary16 = (255*binary).astype( 'uint16' )\n",
        "    \n",
        "    # export as tiff\n",
        "    basename = os.path.basename(file)\n",
        "    tiffdir = rootdir + \"tiff\"\n",
        "    if not os.path.exists(tiffdir):\n",
        "      os.mkdir(tiffdir)\n",
        "    filename = rootdir + \"tiff/\" + basename[:-26] + f\"_p{int(threshold*100)}_all_639.tiff\"\n",
        "    tifffile.imsave( filename, binary16 )\n",
        "    \n",
        "    ### Find center of mass\n",
        "    ids = np.arange( 1, num_objects+1 )\n",
        "    coms = ndi.center_of_mass( binary, objects, ids )\n",
        "    \n",
        "    # convert to numpy array\n",
        "    coms = np.array( coms )\n",
        "    \n",
        "    # Compute volume of each object\n",
        "    unique, counts = np.unique( objects, return_counts=True )\n",
        "    # remove 0\n",
        "    unique = unique[1:]\n",
        "    counts = counts[1:]\n",
        "    \n",
        "    # create empty dataframe\n",
        "    df = pd.DataFrame()\n",
        "    \n",
        "    # colum \"ID\"\n",
        "    df['ID'] = unique\n",
        "    \n",
        "    # column \"X\", \"Y\", \"Z\"\n",
        "    df['X'] = coms[ :, 2 ]\n",
        "    df['Y'] = coms[ :, 1 ]\n",
        "    df['Z'] = coms[ :, 0 ]\n",
        "    \n",
        "    # colum \"volume\"\n",
        "    df[\"volume\"] = counts\n",
        "    \n",
        "    # save as csv\n",
        "    basename = os.path.basename(file)\n",
        "    csvdir = rootdir + \"csv\"\n",
        "    if not os.path.exists(csvdir):\n",
        "      os.mkdir(csvdir)\n",
        "    filename = rootdir + \"csv/\" + basename[:-26] + f\"_p{int(threshold*100)}_all_639.csv\"\n",
        "    df.to_csv( filename, index=False, float_format='%.2f' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bc2b2c2a",
      "metadata": {
        "id": "bc2b2c2a"
      },
      "outputs": [],
      "source": [
        "# Define root diretory\n",
        "rootdir = \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9c9f38c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c9f38c6",
        "outputId": "8c081105-45e4-4c32-bb07-d7d9cd49d1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices.h5']\n"
          ]
        }
      ],
      "source": [
        "# get files which ends with 'probability'\n",
        "file_list = glob.glob( rootdir + \"*_Probabilities_10slices.h5\" )\n",
        "print( file_list )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = file_list[0]\n",
        "prob = load_hdf5( file, \"expmat\", multichannel=False )"
      ],
      "metadata": {
        "id": "-lGLhY7DGom6"
      },
      "id": "-lGLhY7DGom6",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3176aea9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3176aea9",
        "outputId": "ac170ed8-be0f-436d-89b0-ebccaf37931d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 160266768.75\n",
            "Number of detected objects: 235470\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 117569120.625\n",
            "Number of detected objects: 172737\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 84283835.625\n",
            "Number of detected objects: 123833\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 59966465.625\n",
            "Number of detected objects: 88105\n",
            "Data set size: (10, 2160, 2560, 2)\n",
            "190604_P_#144_lung_ctrl_x125_639_Probabilities_10slices\n",
            "(10, 2160, 2560)\n",
            "Total volume of detected signals: 33390781.875\n",
            "Number of detected objects: 49059\n"
          ]
        }
      ],
      "source": [
        "# loop through all files and thresholds\n",
        "thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "for file in file_list:\n",
        "    for thresh in thresholds:\n",
        "      ask_hdf5_size( file, dsetname=None )\n",
        "      raw = load_hdf5(file, multichannel=False)\n",
        "      print (file.rsplit(\"/\")[-1][:-3])\n",
        "      calculate_prob_hdf5(file, thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f916299f",
      "metadata": {
        "id": "f916299f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Signal extraction 10slices.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}